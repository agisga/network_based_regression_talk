<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Network-based penalized regression methods</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/sky.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

    <!-- MathJax interation, more configuration under Reveal.initialize below -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"]]}});
      MathJax.Hub.Config({TeX: {Macros:{subscript:['_{#1}',1],superscript:['^{#1}',1]}}});
    </script> 
	</head>
	<body>
		<div class="reveal">
			<div class="slides">


        <section data-markdown>
          <script type="text/template">
## Network-based penalized regression

### Alexej Gossmann
#### Tulane University
#### Dept. of Global Biostatistics and Data Science
### 2016/11/9
          </script>
        </section>

        <section data-markdown>
          <script type="text/template">
## Background

* Linear model $\mathbf{y}=X\mathbf{b} + \mathbf{z}$, where $\mathbf{y}\in\mathbb{R}^n$, $X\in\mathbb{R}^{n\times p}$, $\mathbf{b}\in\mathbb{R}^p$, $\mathbb{E}(\mathbf{z}) = 0$.
* Possibly $n < p$.
* _Estimation/Prediction_: Find best predictions for $\mathbf{y}$.
* _Feature selection_: Find which $b_i$ are non-zero.
          </script>
        </section>

        <section data-markdown>
          <script type="text/template">
## Motivation

* Typically penalized regression approaches **ignore** any relationships among the features $\mathbf{x}_i$. 
* In biomedical applications features are related.
* A network of relationships between the features $\mathbf{x}_i$
  - can be constructed from the data (e.g., graphical model),
  - may be given as biological prior knowledge (e.g., genetic pathways from KEGG, etc.)

$\leadsto$ Utilize the network in the regression model!
          </script>
        </section>

        <section data-markdown>
          <script type="text/template">
### Currently available methods

1. Bondell and Reich (2008) OSCAR
2. Yang et. al. (2013) GOSCAR
3. Li and Li (2008, 2010) Grace, aGrace
4. Pan et. al. (2010) Incorporating Predictor Network in Penalized Regression with Application to Microarray Data
5. Kim et. al. (2013) Network-based penalized regression with application to genomic data
6. Kim and Xing (2009) GFlasso
7. Zhu et. al. (2013) Simultaneous grouping pursuit and feature selection over an undirected graph
8. Yu and Liu (Oct. 2016) SRIG
          </script>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
### OSCAR

H. Bondell and B. Reich (2008) "Simulataneous Regression Shrinkage, Variable Selection, and Supervised Clustering of Predictors with OSCAR"
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### OSCAR

Octagonal shrinkage and clustering algorithm:
$$\hat{\mathbf{b}} = \arg \min\subscript{\mathbf{b}} \left\lVert \mathbf{y} - X\mathbf{b} \right\rVert_2^2$$
subject to
$$\sum\subscript{j = 1}^p |b_j| + c \sum\subscript{j < k} \max\\{ |b_j|, |b_k| \\} \leq t.$$
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### OSCAR

* $\ell\subscript{1}$ norm encourages sparsity.
* $\ell\subscript{\infty}$ norm encourages equality of coefficients.
* OSCAR encourages grouping of highly correlated variables.
* OSCAR performs sparse regression while simultaneously performing supervised clustering.
* But no networks of graphs envolved!
* Application example: Appalachian Mountains soil data (predicting number of plant species based on soil characteristics).
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### OSCAR

(a) $\rho = 0.15$ (b) $\rho = 0.85$

![Graphical representation of OSCAR](img/OSCAR.png)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### OSCAR

#### An interesting observation 

OSCAR is actually a special case of SLOPE:

$$\arg\min\subscript{\mathbf{b}} \left\lVert \mathbf{y} - X\mathbf{b} \right\rVert_2^2 + \lambda \sum\subscript{j=1}^p (c(p-j) + 1) |b|\subscript{(j)},$$

for $|b|\subscript{(1)} \geq \dots \geq |b|\subscript{(p)}$.
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
### GOSCAR

S. Yang, L. Yuan, Y.-C. Lai, X. Shen, P. Wonka, and J. Ye (2013) "Feature Grouping and Selection Over an Undirected Graph"
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### GOSCAR

* Let $(N, E)$ be the given undirected graph.
* _Assumption_: If nodes $i$ and $j$ are connected by an edge in $E$, then $|b_i|$ and $|b_j|$ tend to be equal.
* Graph OSCAR:
$$\arg\min\subscript{\mathbf{b}} \frac{1}{2} \left\lVert \mathbf{y} - X\mathbf{b} \right\rVert_2^2 + \lambda_1 \\|\mathbf{b} \\|_1 + \lambda_2 \sum\subscript{(i, j)\in E} \max\\{ |b_i|, |b_j| \\}.$$
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### GOSCAR

* When the graph is complete GOSCAR is equivalent to OSCAR.
* GOSCAR is much more challenging to solve than OSCAR.
* GOSCAR encourages equality of absolute values of coefficients for features connected in the graph.
* The $\ell\subscript{\infty}$ regularizer can overpenalize large coefficients, resulting in strongly biased estimates.
* Not clear if robust to graph misspecification.
* Application example: Breast cancer data set ($n = 295$ cancer tumors and $p = 566$ genes), where GOSCAR outperforms LASSO, OSCAR and GFlasso.
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
### Grace and aGrace

C. Li and H. Li (2008) "Network-constrained regularization and variable selection for analysis of genomic data"

C. Li and H. Li (2010) "Variable selection and regression analysis for graph-structured covariates with an application to genomics"

            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Graph-constrained estimation (Grace)

<small>
1. Consider a weighted graph $G = (V, E, W)$.
1. Write $u \sim v$ if predictors $u$ and $v$ are linked in the network.
1. $w(u, v)$ denotes the weight of the edge $e = (u \sim v)$.
1. $d_v := \sum\subscript{(u \sim v)} w(u, v)$ denotes the degree of vertex $v$.
1. The $uv$th element of the normalized Laplacian matrix $L$ is defined as
$$L(u, v) := \begin{cases} 1 - w(u,v) / d_u, \quad \mathrm{if}\, u=v, d_u \neq 0 \\\\ -w(u,v) / \sqrt{d_u d_v}, \quad \mathrm{if}\, u\sim v \\\\ 0, \quad \mathrm{otherwise} \end{cases}.$$
2. The smoothness of vector $\mathbf{b}$ with respect to the graph structure can be expressed as
$$\mathbf{b}^T L \mathbf{b} = \sum\subscript{u\sim v} \left( \frac{b_u}{\sqrt{d_u}} - \frac{b_v}{\sqrt{d_v}} \right)^2 w(u, v).$$
3. A Gaussian Markov random field prior can be assumed for $\mathbf{b}$:
$$f(\mathbf{b}) \propto \exp\left( -\frac{1}{2\sigma^2} \mathbf{b}^T L \mathbf{b} \right).$$

</small>
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Graph-constrained estimation (Grace)

* Grace:
$$\arg\min\subscript{\mathbf{b}} \left\lVert \mathbf{y} - X\mathbf{b} \right\rVert_2^2 + \lambda_1 \\|\mathbf{b} \\|_1 + \lambda_2 \sum\subscript{u\sim v} \left( \frac{b_u}{\sqrt{d_u}} - \frac{b_v}{\sqrt{d_v}} \right)^2 w(u, v).$$
* Similar to the fused lasso (Tibshirani et. al. 2005), but utilizes the network structure and $\ell_2$ norm on the differences.
* The last term penalizes the vector $\mathbf{b}$, if it differs too much over predictors that are linked in the graph.
* What if $u \sim v$, but $b_u$ and $b_v$ have different signs? (e.g., one of two neighboring genes is upregulated while the other is downregulated)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Adaptive Grace

**aGrace**
$$
\begin{eqnarray}
\arg\min\subscript{\mathbf{b}} \left\lVert \mathbf{y} - X\mathbf{b} \right\rVert_2^2 &+& \lambda_1 \\|\mathbf{b} \\|_1 \\\\
&+& \lambda_2 \sum\subscript{u\sim v} \left( \frac{\mathrm{sgn}(\tilde{b}_u) b_u}{\sqrt{d_u}} - \frac{\mathrm{sgn}(\tilde{b}_v) b_v}{\sqrt{d_v}} \right)^2 w(u, v),
\end{eqnarray}
$$
where $\tilde{\mathbf{b}}$ is an initial estimate obtained from LS, Ridge or Enet (2-step procedure).
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Grace application example

* Glioblastoma microarray gene-expression data.
* $n = 50$ patients in the training data, $n = 61$ patients in the test data.
* $p = 1533$ genes, organized in a network of 33 KEGG pathways.
* Logarithm of time to death used as the response variable.
* Grace outperforms LASSO and Enet.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### aGrace application example

* Analysis of gene expression data measured in human brains of individuals of different ages.
* Logarithm of age of $n = 30$ individuals as the response variable.
* Expression levels of $p = 1305$ genes as the predictors.
* KEGG network with 5288 edges.
* aGrace outperforms LASSO, Enet, and Grace.
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
W. Pan, B. Xie, X. Shen (2010) "Incorporating Predictor Network in Penalized Regression with Application to Microarray Data"
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
* $L\subscript{\gamma}$ penalized regression (class of penalties): 

$$\arg\min\subscript{\mathbf{b}} \left\lVert \mathbf{y} - X\mathbf{b} \right\rVert_2^2 + \lambda 2^{\frac{1}{\gamma^\prime}} \sum\subscript{i\sim j} \left( \frac{|b_i|^{\gamma}}{w_i} + \frac{|b_j|^{\gamma}}{w_j} \right)^{\frac{1}{\gamma}},$$
* $1/\gamma^{\prime} + 1/\gamma = 1$ and $\gamma > 1$.
* Each term is a weighted group penalty $\Rightarrow$ connected features are likely to have similar effects.
* $w_i$ determines what to smooth:
  - $w_i = d_i$ encourages $|b_i| \approx |b_j|$ if $i\sim j$.
  - $w_i = d_i^{(\gamma + 1) / 2}$ encourages $\frac{|b_i|}{\sqrt{d_i}} \approx \frac{|b_j|}{\sqrt{d_j}}$ if $i\sim j$.
* Larger $\gamma$ $\Rightarrow$ more smoothing.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
$L\subscript{\gamma}$-norm ball:

![L_gamma norm](img/Pan_et_al.png)

Larger $\gamma$ $\Rightarrow$ more smoothing.
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
* S. Kim, W. Pan, and X. Shen (2013) "Network-based penalized regression with application to genomic data"
* Y. Zhu, X. Shen, and W. Pan (2013) "Simultaneous grouping and feature selection over an undirected graph"
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
* All previously shown methods assume that $\frac{|b_i|}{w_i} \approx \frac{|b_j|}{w_j}$ if $i\sim j$.
* Too strong an assumption? 
* Relaxation: $b_i$ and $b_j$ are likely to be zero or non-zero at the same time (if $i \sim j$).
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
<small>
* Prior assumption: $I(b_i \neq 0) = I(b_j \neq 0)$ if $i \sim j$.
* Truncated Lasso Penalty (Shen et. al. 2012):

$$J\subscript{\tau}(|z|) = \min\left( \frac{|z|}{\tau}, 1 \right) \rightarrow I(z \neq 0), \quad \mathrm{as}\, \tau \to 0^+.$$

</small>
![TLP](img/TLP.png)
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
* $TTLP_I$
$$
\begin{eqnarray}
\arg\min\subscript{\mathbf{b}} \left\lVert \mathbf{y} - X\mathbf{b} \right\rVert_2^2 &+& \lambda_1 \sum\subscript{i = 1}^p J\subscript{\tau}(|b_i|) \\\\
&+& \lambda_2 \sum\subscript{i\sim j} \left\lvert J\subscript{\tau}\left(\frac{|b_i|}{w_i}\right) - J\subscript{\tau}\left(\frac{|b_j|}{w_j}\right) \right\rvert,
\end{eqnarray}
$$
* $LTLP_I$
$$
\begin{eqnarray}
\arg\min\subscript{\mathbf{b}} \left\lVert \mathbf{y} - X\mathbf{b} \right\rVert_2^2 &+& \lambda_1 \sum\subscript{i = 1}^p |b_i| \\\\
&+& \lambda_2 \sum\subscript{i\sim j} \left\lvert J\subscript{\tau}\left(\frac{|b_i|}{w_i}\right) - J\subscript{\tau}\left(\frac{|b_j|}{w_j}\right) \right\rvert,
\end{eqnarray}
$$
* Non-convex (use difference convex programming).

            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
* Simulation studies:
  - $TTLP_I$ and $LTLP_I$ produce less biased estimates than Grace, aGrace, and $\ell\subscript{\infty}$ based methods.
  - $TTLP_I$ and $LTLP_I$ are robust to misspecified weights and misspecified network, when compared to Grace, aGrace, and $\ell\subscript{\infty}$ based methods.
* Breast cancer gene expression data:
  - $n = 286 + 295$ patients from two studies.
  - Binary outcome variable (metastasis).
  - Prior gene network of $p = 294$ genes and 326 edges.
* eQTL data
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
### GFlasso

S. Kim and E. P. Xing (2009) "Statistical Estimation of Correlated Genome Associations to a Quantitative Train Network"
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Graph-guided fused lasso for multiple correlated traits (GFlasso)

* Captures a network among multiple response variables:

$$
\begin{eqnarray}
\arg\min\subscript{\mathbf{b}} \sum_k \left\lVert \mathbf{y}_k - X\mathbf{b}_k \right\rVert_2^2 &+& \lambda_1 \sum_k \sum\subscript{i = 1}^p |b\subscript{ki}| \\\\
&+& \lambda_2 \sum\subscript{m\sim l} \sum\subscript{i = 1}^p | b\subscript{mi} - \mathrm{sgn}(\rho\subscript{ml}) b\subscript{li} |.
\end{eqnarray}
$$

* Utilizes a quantitative trait network in a multivariate regression model, in order to identify pleiotropic genes/SNPs.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### GFlasso

* When there is only one response variable, GFlasso can be used to capture a network between features:

$$
\begin{eqnarray}
\arg\min\subscript{\mathbf{b}} \left\lVert \mathbf{y} - X\mathbf{b} \right\rVert_2^2 + \lambda_1 \\|\mathbf{b}\\|_1 + \lambda_2 \sum\subscript{i\sim j} | b\subscript{i} - \mathrm{sgn}(\rho\subscript{ij}) b\subscript{j} |.
\end{eqnarray}
$$
            </script>
          </section>
        </section>

        <section>
          <section data-markdown>
            <script type="text/template">
### SRIG

G. Yu and Y. Liu (2016) "Sparse Regression Incorporating Graphical Structure Among Predictors"
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### SRIG

* $\mathbf{y}=X\mathbf{b} + \mathbf{z}$, where $\mathbf{y}\in\mathbb{R}^n$, $X = [X_1, X_2, \dots, X_p] \in\mathbb{R}^{n\times p}$, $\mathbf{b}\in\mathbb{R}^p$, $\mathbb{E}(\mathbf{z}) = 0$.
* Predictor graph $G$.
* *Random design setting*: For each *row* $\mathbf{x}_i$ of $X$ assume that $\mathbb{E}(\mathbf{x}_i) = 0$ and $\mathbb{Var}(\mathbf{x}_i) = \Sigma$.
* Denote $\Omega = (\omega\subscript{ij}) := \Sigma^{-1}$.
* Denote $\Sigma\subscript{xy} = (c_1, c_2, \dots, c_p)^T := \mathbb{Cov}(X_k, y_k)$.

$$\Rightarrow \Sigma\subscript{xy} = \mathbb{E}(X^T Y / n) = \mathbb{E}(X^T X \mathbf{b} / n) + \mathbf{E}(X^T \mathbf{z} / n) = \Sigma \mathbf{b}$$

$$\Rightarrow \mathbf{b} = \Sigma^{-1} \Sigma\subscript{xy} = \Omega \Sigma\subscript{xy}$$
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### SRIG

* From $\mathbf{b} = \Omega \Sigma\subscript{xy}$ we have that
$$
\begin{eqnarray}
b_1 &=& c_1 \omega\subscript{11} + c_2 \omega\subscript{12} + \dots + c_p \omega\subscript{1p} \\\\
b_2 &=& c_1 \omega\subscript{21} + c_2 \omega\subscript{22} + \dots + c_p \omega\subscript{2p} \\\\
\vdots \\\\
b_p &=& c_1 \omega\subscript{p1} + c_2 \omega\subscript{p2} + \dots + c_p \omega\subscript{pp} \\\\
\end{eqnarray}
$$
* Notice that $\mathbf{b}$ consists of $p$ additive parts.
* If $X_i$ is uncorrelated with $\mathbf{y}$, then $c_i = 0$ and $(c_1 \omega\subscript{1}, c_2 \omega\subscript{2}, \dots, c_p \omega\subscript{p}) =  0$.
* If $c_i \neq 0$, then the support of $(c_i \omega\subscript{1i}, c_i \omega\subscript{2i}, \dots, c_i \omega\subscript{pi})$ is determined by the neighborhood of node $i$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### SRIG

* Adjacency matrix $E$ (0-1-valued, convention: $E\subscript{i,i} = 1$).
* $\mathcal{N}_i := \\{j: E\subscript{ij} \neq 0\\}$ = "$i$th node and its neighbors".
* Change of variables:
$$
\begin{eqnarray}
b_1 &=& V_1^{(1)} E\subscript{11} + V_1^{(2)} E\subscript{12} + \dots + V_1^{(p)} E\subscript{1p} \\\\
b_2 &=& V_2^{(1)} E\subscript{21} + V_2^{(2)} E\subscript{22} + \dots + V_2^{(p)} E\subscript{2p} \\\\
\vdots \\\\
b_p &=& V_p^{(1)} E\subscript{p1} + V_p^{(2)} E\subscript{p2} + \dots + V_p^{(p)} E\subscript{pp} \\\\
\end{eqnarray}
$$

$\leadsto$ Decomposition $\mathbf{b} = V^{(1)} + V^{(2)} + \dots + V^{(p)}$, where $V^{(i)} = 0$ for some $i$, and $\mathrm{supp}(V^{(i)}) \subset \mathcal{N}_i$ for all $i$.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
#### Sparse Regression Incorporating Graphical structure among predictors (SRIG)

$$\min\subscript{\mathbf{b}, V^{(1)}, \dots, V^{(p)}} \frac{1}{2n} \left\lVert \mathbf{y} - X\mathbf{b} \right\rVert_2^2 + \lambda \sum\subscript{i = 1}^p \tau_i \left\lVert V^{(i)} \right\rVert_2,$$
subject to
$$\sum\subscript{i = 1}^p V^{(i)} = \mathbf{b},$$
$$\mathrm{supp}(V^{(i)}) \subseteq \mathcal{N}_i,\quad \forall i = 1,\dots,p.$$

            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### SRIG

* Node-by-node rather than edge-by-edge.
* Adaptive LASSO (no edges in $G$), group LASSO ($G$ consists of disconnected complete subgraphs), and Ridge Regression ($G$ is a complete graph) are special cases.
* Theoretical finite sample bounds for prediction and estimation.
* Model selection consistency.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### SRIG

* Simulation results
  - SRIG performs well for estimation, prediction, and feature selection.
  - SRIG generally outperforms LS, LASSO, aLASSO, Ridge, Enet, PCR, SPLS, GOSCAR, GRACE, under the assumption that connected variables in the graph act together.
  - If the intersection between the neighborhoods of relevant and irrelevant predictors is big, then LASSO outperforms SRIG.
            </script>
          </section>

          <section data-markdown>
            <script type="text/template">
### SRIG

#### Alzheimer's Disease Neuroimaging Initiative (ADNI)
 
* Mini Mental State Examination (MMSE) score (0-30 points) is predicted from structural MRI.
* 51 AD patients, 52 controls; total: $n = 103$.
* 93 regions of interest (ROI); for each ROI, volume of GM tissue used as a feature; total $p = 93$.
* $G$ estimated by the graphical Lasso (Friedman et. al. 2008), has 419 edges.
* SRIG outperforms LASSO, Ridge, aLasso, Enet, GOSCAR, GRACE, PCR, SPLS in terms of MSE on test data.
            </script>
          </section>
        </section>

			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				history: true,
        slideNumber: true,

				transition: 'concave', // none/fade/slide/convex/concave/zoom

        // MathJax integration
        math: {
            mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },

          // MathJax
          { src: 'plugin/math/math.js', async: true },

					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
